{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx3ef9NWYDWF",
        "outputId": "6161d878-ba33-4d89-e2be-a0e0e308c309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jt4BGWGJXOZy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import splitfolders\n",
        "import cv2\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras import layers, models\n",
        "import kagglehub\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YT-NrsYuoWLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d6fe461-7107-420b-fd7b-7080ec584703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/thedevastator/anime-face-dataset-by-character-name?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 57.9M/57.9M [00:00<00:00, 95.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#constraints\n",
        "#classes\n",
        "max_classes_num = 130\n",
        "\n",
        "#images\n",
        "img_size = (224,224)\n",
        "batch_size = 32\n",
        "\n",
        "#files settings paths, model/classes files\n",
        "download_dir = kagglehub.dataset_download(\"thedevastator/anime-face-dataset-by-character-name\")\n",
        "input_dir = \"/kaggle/modifiedInput/AnimeFaceRecognition\"\n",
        "output_dir = \"/kaggle/output/AnimeFaceRecognition\"\n",
        "dataset_source_dir = os.path.join(download_dir, \"dataset\")\n",
        "\n",
        "#variables\n",
        "#classes\n",
        "classes_num = 5\n",
        "\n",
        "#model\n",
        "epochs_num = 25\n",
        "\n",
        "#files settings paths, model/classes files\n",
        "model_name = \"anime_face_recognition_model\"\n",
        "model_output_dir = \"/kaggle/output/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "vmDZB2tnYYQw",
        "outputId": "e93ebf41-ee66-4c5b-cd1e-ae1be76e309c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded successfully in /root/.cache/kagglehub/datasets/thedevastator/anime-face-dataset-by-character-name/versions/1/dataset\n",
            "Copied asia_argento class\n",
            "Copied formidable_(azur_lane) class\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/kaggle/modifiedInput/AnimeFaceRecognition/dataset/laffey_(azur_lane)'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c38215717d89>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mclass_target_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_target_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_source_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_target_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Copied {class_name} class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    574\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muse_srcentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/kaggle/modifiedInput/AnimeFaceRecognition/dataset/laffey_(azur_lane)'"
          ]
        }
      ],
      "source": [
        "if os.path.exists(dataset_source_dir):\n",
        "  print(f\"Dataset downloaded successfully in {dataset_source_dir}\")\n",
        "\n",
        "  os.makedirs(input_dir, exist_ok=True)\n",
        "  dataset_target_dir = os.path.join(input_dir, \"dataset\")\n",
        "  os.makedirs(dataset_target_dir, exist_ok=True)\n",
        "  os.makedirs(model_output_dir, exist_ok=True)\n",
        "\n",
        "  all_classes = os.listdir(dataset_source_dir)\n",
        "  selected_classes = random.sample(all_classes, classes_num % max_classes_num)\n",
        "  selected_classes.sort()\n",
        "\n",
        "  for class_name in selected_classes:\n",
        "    class_source_dir = os.path.join(dataset_source_dir, class_name)\n",
        "    class_target_dir = os.path.join(dataset_target_dir, class_name)\n",
        "\n",
        "    shutil.copytree(class_source_dir, class_target_dir)\n",
        "    print(f\"Copied {class_name} class\")\n",
        "\n",
        "  with open(output_dir + \"/\" + model_name + \"_class_names.pkl\", \"wb\") as f:\n",
        "    pickle.dump(selected_classes, f)\n",
        "    print(f\"Class names saved to: {model_output_dir}\")\n",
        "\n",
        "  print(\"Dataset copied successfully\")\n",
        "\n",
        "  split_ratio = (0.8, 0.1, 0.1)\n",
        "\n",
        "  splitfolders.ratio(\n",
        "      dataset_target_dir,\n",
        "      output=output_dir,\n",
        "      seed=500,\n",
        "      ratio=split_ratio,\n",
        "      group_prefix=None\n",
        "  )\n",
        "else:\n",
        "  print(\"Dataset download failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UzwlHARa8NI"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "  preprocessing_function=preprocess_input,\n",
        "  rotation_range=20,\n",
        "  width_shift_range=0.2,\n",
        "  height_shift_range=0.2,\n",
        "  shear_range=0.2,\n",
        "  zoom_range=0.2,\n",
        "  horizontal_flip=True,\n",
        "  fill_mode='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcjvjHBib4ax"
      },
      "outputs": [],
      "source": [
        "#Data augmentation for test data\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "#Data augmentation form validation data\n",
        "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VSE8zUZcZ_u"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(output_dir, \"train\")\n",
        "test_dir = os.path.join(output_dir, \"test\")\n",
        "valid_dir = os.path.join(output_dir, \"val\")\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "  train_dir,\n",
        "  target_size=img_size,\n",
        "  batch_size=batch_size,\n",
        "  class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "  test_dir,\n",
        "  target_size=img_size,\n",
        "  batch_size=batch_size,\n",
        "  class_mode='categorical'\n",
        ")\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(\n",
        "  valid_dir,\n",
        "  target_size=img_size,\n",
        "  batch_size=batch_size,\n",
        "  class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55SFpYl-c9xD"
      },
      "outputs": [],
      "source": [
        "#batch of images and labels\n",
        "images, labels = next(valid_data)\n",
        "\n",
        "#select a random image from the batch\n",
        "idx = np.random.randint(0, images.shape[0] - 1)\n",
        "\n",
        "plt.imshow(images[idx])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q2Krb_sddX2"
      },
      "outputs": [],
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "#freeze the convolutional base\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKucOsrcf6QG"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_data.num_classes, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg9j9n9jgtaa"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oach_EPOhNYH"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_data, epochs=epochs_num, validation_data=valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFneepBtpb9S"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IPI3D5NqlmG"
      },
      "outputs": [],
      "source": [
        "def predict_img(image, model):\n",
        "  test_img = cv2.imread(image)\n",
        "  test_img = cv2.resize(test_img, img_size)\n",
        "  test_img = np.expand_dims(test_img, axis=0)\n",
        "  result=model.predict(test_img)\n",
        "  r=np.argmax(result)\n",
        "  print(selected_classes[r])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S1XWLgYuIm2"
      },
      "outputs": [],
      "source": [
        "#predict_img(\"/kaggle/output/AnimeFaceRecognition/test/shirogane_naoto/\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYk6biwEbZC9"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "    ax1.set_title('Model Accuracy Over Epochs')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "    ax2.set_title('Model Loss Over Epochs')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def generate_confusion_matrix(model, test_data, class_names):\n",
        "    # Get predictions\n",
        "    predictions = model.predict(test_data)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Get true labels\n",
        "    true_classes = test_data.classes\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return cm, predicted_classes, true_classes\n",
        "\n",
        "def generate_classification_report(true_classes, predicted_classes, class_names):\n",
        "    report = classification_report(true_classes, predicted_classes,\n",
        "                                 target_names=class_names, output_dict=True)\n",
        "\n",
        "    # Convert to DataFrame for better visualization\n",
        "    import pandas as pd\n",
        "    df_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(df_report.round(3))\n",
        "\n",
        "    return df_report\n",
        "\n",
        "def visualize_predictions(model, test_data, class_names, num_images=9):\n",
        "    # Get a batch of test images\n",
        "    test_images, test_labels = next(test_data)\n",
        "    predictions = model.predict(test_images)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "    # Plot predictions\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i in range(min(num_images, len(test_images))):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "\n",
        "        # Preprocess image for display\n",
        "        img = test_images[i]\n",
        "        if img.max() <= 1.0:\n",
        "            img = (img * 255).astype('uint8')\n",
        "\n",
        "        plt.imshow(img)\n",
        "\n",
        "        # Determine color based on prediction correctness\n",
        "        color = 'green' if predicted_classes[i] == true_classes[i] else 'red'\n",
        "        confidence = predictions[i][predicted_classes[i]] * 100\n",
        "\n",
        "        plt.title(f'True: {class_names[true_classes[i]]}\\n'\n",
        "                 f'Pred: {class_names[predicted_classes[i]]}\\n'\n",
        "                 f'Conf: {confidence:.1f}%', color=color)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n9M6vJxb3_R"
      },
      "outputs": [],
      "source": [
        "print(\"Generating training history plots...\")\n",
        "plot_training_history(history)\n",
        "\n",
        "print(\"\\nGenerating confusion matrix...\")\n",
        "cm, predicted_classes, true_classes = generate_confusion_matrix(model, test_data, selected_classes)\n",
        "\n",
        "print(\"\\nGenerating classification report...\")\n",
        "report_df = generate_classification_report(true_classes, predicted_classes, selected_classes)\n",
        "\n",
        "print(\"\\nVisualizing sample predictions...\")\n",
        "# Reset test_data generator\n",
        "test_data.reset()\n",
        "visualize_predictions(model, test_data, selected_classes, num_images=9)\n",
        "\n",
        "# Additional metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total test samples: {len(true_classes)}\")\n",
        "print(f\"Number of classes: {len(selected_classes)}\")\n",
        "print(f\"Overall accuracy: {(predicted_classes == true_classes).mean():.4f}\")\n",
        "\n",
        "# Per-class accuracy\n",
        "for i, class_name in enumerate(selected_classes):\n",
        "    class_mask = true_classes == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_accuracy = (predicted_classes[class_mask] == true_classes[class_mask]).mean()\n",
        "        print(f\"{class_name}: {class_accuracy:.4f} ({class_mask.sum()} samples)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoIe4XRUreLK"
      },
      "outputs": [],
      "source": [
        "model.save(model_output_dir + \"/\" + model_name + \".h5\")\n",
        "model.save(model_output_dir + \"/\" + model_name + \".keras\")\n",
        "\n",
        "print(f\"Model saved to {model_output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}